from abc import ABC, abstractmethod
from transformers import AutoTokenizer
import torch

DEVICE = 0 if torch.cuda.is_available() else 'cpu'


class BaseRewardModel(ABC):
    '''
    Inherit this class and implement _predict method
    _predict takes two arguments:
        inputs: raw model input
        responses: list of responses (str) generated by base model
    '''

    def predict(self, inputs, responses):
        scores = self._predict(inputs, responses)
        self._check_prediction_output(scores)
        return scores

    def _check_prediction_output(self, scores):
        assert isinstance(scores, list)
        for score in scores:
            assert isinstance(score, dict)
            assert 'text' in score
            assert 'score' in score

    @abstractmethod
    def _predict(self, inputs, responses):
        pass


class _GPT2MixIn():
    def _load_tokenizer(self):
        tokenizer = AutoTokenizer.from_pretrained(
                'gpt2',
                truncation_side='left',
                padding_side='right'
                )
        tokenizer.pad_token_id = 50256
        return tokenizer

    def _tokenize(self, text):
        output = self.tokenizer(
            text,
            return_tensors='pt',
            return_attention_mask=True,
            padding='longest',
            truncation=True,
            max_length=256
            ).to(DEVICE)
        return output
