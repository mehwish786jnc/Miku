from dataclasses import dataclass, field
from typing import Optional
from transformers import MODEL_FOR_CAUSAL_LM_MAPPING

MODEL_CONFIG_CLASSES = list(MODEL_FOR_CAUSAL_LM_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)

@dataclass
class ModelArguments:
    """
    Arguments related to model configuration and initialization.
    """
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "The model checkpoint for weights initialization. Leave unset if training a model from scratch."
        },
    )
    model_type: Optional[str] = field(
        default=None,
        metadata={"help": "Specify the model type if training from scratch: " + ", ".join(MODEL_TYPES)},
    )
    config_overrides: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "Override default config settings when training a model from scratch. "
                "Example: n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
            )
        },
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Name or path of the pretrained config if different from model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Name or path of the pretrained tokenizer if different from model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Directory to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Use fast tokenizer backed by the tokenizers library."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "Specify model version to use (branch name, tag name, or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={"help": "Use token generated by `huggingface-cli login` for private models."},
    )

    def __post_init__(self):
        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):
            raise ValueError("--config_overrides cannot be used with --config_name or --model_name_or_path")

@dataclass
class DataTrainingArguments:
    """
    Arguments related to data input for training and evaluation.
    """
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(
        default=None, metadata={"help": "Path to the training data file (a text file)."}
    )
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "Optional evaluation data file to evaluate perplexity on (a text file)."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={"help": "For debugging or quick training, limit the number of training examples."},
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={"help": "For debugging or quick evaluation, limit the number of evaluation examples."},
    )
    block_size: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "Optional sequence length after tokenization. "
                "The dataset will be truncated into blocks of this size for training. "
                "Defaults to the model max input length for single sentence inputs (including special tokens)."
            )
        },
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[int] = field(
        default=5,
        metadata={"help": "Percentage of the train set used as validation set if no validation split is available."},
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "Number of processes to use for preprocessing."},
    )
    keep_linebreaks: bool = field(
        default=True, metadata={"help": "Whether to keep line breaks when using TXT files."}
    )
    eval_prompt_path: Optional[str] = field(
        default=None, metadata={"help": "Path to prompt completions for logging."}
    )
    num_eval_prompts: Optional[int] = field(
        default=1024, metadata={"help": "Number of evaluation prompts to check."}
    )
    hellaswag_sample_size: Optional[int] = field(
        default=None, metadata={"help": "Number of hellaswag samples for generating metrics."}
    )
    lambada_enabled: Optional[bool] = field(
        default=None, metadata={"help": "Whether to log lambada metrics."}
    )
    add_reward_scores: Optional[bool] = field(
        default=False, metadata={"help": "Whether to log reward model scores."}
    )
    clean_enabled: Optional[bool] = field(
        default=False, metadata={"help": "Delete optimizer after saving checkpoint (saves space, prevents resuming)."}
    )
    eval_first_step: Optional[bool] = field(
        default=False, metadata={"help": "Evaluate the model before training begins."}
    )
    train_to_probs: Optional[bool] = field(
        default=False, metadata={"help": "Train against soft targets."}
    )

    def __post_init__(self):
        if self.dataset_name is None and self.train_file is None and self.validation_file is None:
            raise ValueError("Either dataset_name, train_file, or validation_file must be specified.")
        if self.train_file is not None:
            extension = self.train_file.split(".")[-1]
            assert extension in ["csv", "json", "txt"], "`train_file` must be a csv, json, or txt file."
        if self.validation_file is not None:
            extension = self.validation_file.split(".")[-1]
            assert extension in ["csv", "json", "txt"], "`validation_file` must be a csv, json, or txt file."
